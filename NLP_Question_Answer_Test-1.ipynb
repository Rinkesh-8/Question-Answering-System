{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e6f571",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40428020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb569c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the SQuAD dataset\n",
    "def download_squad_data():\n",
    "    #url = \"https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/train-v1.1.json\"\n",
    "    url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"  # SQuAD 1.1 URL\n",
    "    response = requests.get(url)\n",
    "    squad_data = response.json()\n",
    "    return squad_data\n",
    "\n",
    "# Function to preprocess the SQuAD data and create QA pairs\n",
    "def preprocess_squad_data(squad_data, num_examples):  # You can adjust the number of examples as needed\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    qa_pairs = []\n",
    "    for article in squad_data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qas in paragraph[\"qas\"]:\n",
    "                question = qas[\"question\"]\n",
    "                if qas[\"answers\"]:\n",
    "                    answer_text = qas[\"answers\"][0][\"text\"]  # We'll consider only the first answer\n",
    "                    answer_start = qas[\"answers\"][0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                else:\n",
    "                    # If there are no answers, set them to -1\n",
    "                    answer_text = \"\"\n",
    "                    answer_start = -1\n",
    "                    answer_end = -1\n",
    "                \n",
    "                # Tokenize the context and question\n",
    "                inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\", max_length=750,truncation=True)\n",
    "                input_ids = inputs[\"input_ids\"].flatten()\n",
    "                attention_mask = inputs[\"attention_mask\"].flatten()\n",
    "                \n",
    "                qa_pairs.append({\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"answer_text\": answer_text,\n",
    "                    \"answer_start\": answer_start,\n",
    "                    \"answer_end\": answer_end,\n",
    "                    \"input_ids\": input_ids.tolist(),\n",
    "                    \"attention_mask\": attention_mask.tolist()\n",
    "                })\n",
    "                \n",
    "                # Stop after processing the desired number of examples\n",
    "                if len(qa_pairs) >= num_examples:\n",
    "                    return qa_pairs\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Download the SQuAD data\n",
    "squad_data = download_squad_data()\n",
    "\n",
    "# Preprocess a smaller subset of the SQuAD data and create QA pairs\n",
    "num_examples = 5000  # Change this number to control the size of the dataset\n",
    "qa_pairs = preprocess_squad_data(squad_data, num_examples)\n",
    "\n",
    "# Optional: Save the preprocessed QA pairs to a JSON file\n",
    "with open(\"squad_lite_qa_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc4b80",
   "metadata": {},
   "source": [
    "Tokenization data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca1f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization using BERT tokenizer\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Tokenize the QA pairs for model input\n",
    "tokenized_qa_pairs = []\n",
    "max_length = 750 # Set the maximum sequence length\n",
    "\n",
    "for qa_pair in qa_pairs:\n",
    "    # Tokenize context and question separately\n",
    "    tokenized_context = tokenizer.encode(qa_pair[\"context\"], add_special_tokens=False,max_length=750,truncation=True)\n",
    "    tokenized_question = tokenizer.encode(qa_pair[\"question\"], add_special_tokens=False,max_length=750,truncation=True)\n",
    "\n",
    "    # Combine context and question tokens with [SEP] token in between\n",
    "    input_ids = tokenized_context + [tokenizer.sep_token_id] + tokenized_question\n",
    "\n",
    "    # Create attention mask where 1's indicate tokens and 0's indicate padding\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Ensure the input sequence is within model's maximum length\n",
    "    if len(input_ids) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "\n",
    "    # Padding\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "    attention_mask = attention_mask + [0] * padding_length\n",
    "\n",
    "    # Record the start and end position of the answer within the tokenized input\n",
    "    answer_start = min(qa_pair[\"answer_start\"], max_length - 1)\n",
    "    answer_end = min(qa_pair[\"answer_end\"], max_length - 1)\n",
    "\n",
    "    tokenized_qa_pairs.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"answer_start\": answer_start,\n",
    "        \"answer_end\": answer_end\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e468e",
   "metadata": {},
   "source": [
    "Fine-Tuning Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9367b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a005a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Fine-tuning the Model\n",
    " \n",
    "# Define a custom dataset class for tokenized QA pairs\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_pairs):\n",
    "        self.qa_pairs = qa_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.qa_pairs[idx][\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(self.qa_pairs[idx][\"attention_mask\"]),\n",
    "            \"start_positions\": torch.tensor(self.qa_pairs[idx][\"answer_start\"]),\n",
    "            \"end_positions\": torch.tensor(self.qa_pairs[idx][\"answer_end\"])\n",
    "        }\n",
    "    \n",
    "\n",
    "# Prepare the fine-tuning data\n",
    "train_size = int(0.8 * len(tokenized_qa_pairs))\n",
    "train_dataset = QADataset(tokenized_qa_pairs[:train_size])  # tokenized_qa_pairs from Step 2\n",
    "val_dataset = QADataset(tokenized_qa_pairs[train_size:])\n",
    "\n",
    "\n",
    "# Define batch size and create DataLoaders\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9048798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1022fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "S:\\Python-Anaconda\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:12:51<00:00,  8.74s/it]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:11:38<00:00,  8.60s/it]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:11:32<00:00,  8.59s/it]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:13:30<00:00,  8.82s/it]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:11:24<00:00,  8.57s/it]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████| 500/500 [1:11:17<00:00,  8.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_qa_model\\\\tokenizer_config.json',\n",
       " 'fine_tuned_qa_model\\\\special_tokens_map.json',\n",
       " 'fine_tuned_qa_model\\\\vocab.txt',\n",
       " 'fine_tuned_qa_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained BERT model for Question Answering and move it to GPU\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * 10  # You can change the number of epochs (here 3)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(6):  \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_qa_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_qa_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387c183",
   "metadata": {},
   "source": [
    "Evaluation and Hyperparameter\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1318b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 125/125 [12:40<00:00,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.509262018203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"fine_tuned_qa_model\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the validation dataset\n",
    "val_dataset = QADataset(tokenized_qa_pairs[train_size:])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_loss = evaluate_model(model, val_dataloader)\n",
    "\n",
    "print(\"Validation Loss:\", validation_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474d254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Python-Anaconda\\Anaconda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: what is the elevation of mount everest? mount everest is the highest peak in the world, with an elevation of 8, 848 meters ( 29, 029\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"fine_tuned_qa_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fine_tuned_qa_model\")\n",
    "\n",
    "# Function to get an answer from the model\n",
    "def get_answer(context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_probs = torch.softmax(outputs.start_logits, dim=1)\n",
    "    end_probs = torch.softmax(outputs.end_logits, dim=1)\n",
    "\n",
    "    start_index = torch.argmax(start_probs)\n",
    "    end_index = torch.argmax(end_probs)\n",
    "\n",
    "    # Handle cases where the end index is before the start index\n",
    "    if end_index < start_index:\n",
    "        start_index, end_index = end_index, start_index\n",
    "\n",
    "    answer_tokens = input_ids[0, start_index:end_index+1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"Mount Everest is the highest peak in the world, with an elevation of 8,848 meters (29,029 feet). It is located in the Himalayas.\"\n",
    "question = \"What is the elevation of Mount Everest?\"\n",
    "answer = get_answer(context, question)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
